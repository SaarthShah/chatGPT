{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5db23eca",
   "metadata": {},
   "source": [
    "# GPT from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10a8065",
   "metadata": {},
   "source": [
    "Trying to jump in a bit deeper on language models, this code is just me studying from a video that Karpathy made:\n",
    "https://youtu.be/kCc8FmEb1nY?si=4S4voHUOZv2Dyrn1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21a84608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ff195b",
   "metadata": {},
   "source": [
    "### Getting and cleaning the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "252fc202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-08-22 23:34:09--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8000::154, 2606:50c0:8001::154, 2606:50c0:8002::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8000::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt’\n",
      "\n",
      "input.txt           100%[===================>]   1.06M   295KB/s    in 3.7s    \n",
      "\n",
      "2024-08-22 23:34:19 (295 KB/s) - ‘input.txt’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a815b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ec7c546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202651\n"
     ]
    }
   ],
   "source": [
    "print(len(text.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2051f896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c774f7a2",
   "metadata": {},
   "source": [
    "### Encoder and Decoder at Character Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "507eb5b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42, 2]\n",
      "hello world!\n"
     ]
    }
   ],
   "source": [
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda string: [stoi[ch] for ch in string]\n",
    "decode = lambda string: ''.join([itos[ch] for ch in string])\n",
    "\n",
    "print(encode('hello world!'))\n",
    "print(decode(encode('hello world!')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd946f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "# enoding the entire text dataset into torch.Tensor\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0249dd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting into training and validation\n",
    "n = int(0.9* len(data))\n",
    "train = data[:n]\n",
    "validation = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48770cc",
   "metadata": {},
   "source": [
    "### Training the transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b7b4355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are 8 individual patterns packed here, which explains the +1\n",
    "block_size = 8\n",
    "train[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da4f0a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) then the target:47\n",
      "when input is tensor([18, 47]) then the target:56\n",
      "when input is tensor([18, 47, 56]) then the target:57\n",
      "when input is tensor([18, 47, 56, 57]) then the target:58\n",
      "when input is tensor([18, 47, 56, 57, 58]) then the target:1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) then the target:15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) then the target:47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) then the target:58\n"
     ]
    }
   ],
   "source": [
    "x = train[:block_size]\n",
    "y = train[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} then the target:{target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7869a2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "----\n",
      "when input is [24] then the target:43\n",
      "when input is [24, 43] then the target:58\n",
      "when input is [24, 43, 58] then the target:5\n",
      "when input is [24, 43, 58, 5] then the target:57\n",
      "when input is [24, 43, 58, 5, 57] then the target:1\n",
      "when input is [24, 43, 58, 5, 57, 1] then the target:46\n",
      "when input is [24, 43, 58, 5, 57, 1, 46] then the target:43\n",
      "when input is [24, 43, 58, 5, 57, 1, 46, 43] then the target:39\n",
      "when input is [44] then the target:53\n",
      "when input is [44, 53] then the target:56\n",
      "when input is [44, 53, 56] then the target:1\n",
      "when input is [44, 53, 56, 1] then the target:58\n",
      "when input is [44, 53, 56, 1, 58] then the target:46\n",
      "when input is [44, 53, 56, 1, 58, 46] then the target:39\n",
      "when input is [44, 53, 56, 1, 58, 46, 39] then the target:58\n",
      "when input is [44, 53, 56, 1, 58, 46, 39, 58] then the target:1\n",
      "when input is [52] then the target:58\n",
      "when input is [52, 58] then the target:1\n",
      "when input is [52, 58, 1] then the target:58\n",
      "when input is [52, 58, 1, 58] then the target:46\n",
      "when input is [52, 58, 1, 58, 46] then the target:39\n",
      "when input is [52, 58, 1, 58, 46, 39] then the target:58\n",
      "when input is [52, 58, 1, 58, 46, 39, 58] then the target:1\n",
      "when input is [52, 58, 1, 58, 46, 39, 58, 1] then the target:46\n",
      "when input is [25] then the target:17\n",
      "when input is [25, 17] then the target:27\n",
      "when input is [25, 17, 27] then the target:10\n",
      "when input is [25, 17, 27, 10] then the target:0\n",
      "when input is [25, 17, 27, 10, 0] then the target:21\n",
      "when input is [25, 17, 27, 10, 0, 21] then the target:1\n",
      "when input is [25, 17, 27, 10, 0, 21, 1] then the target:54\n",
      "when input is [25, 17, 27, 10, 0, 21, 1, 54] then the target:39\n"
     ]
    }
   ],
   "source": [
    "# Batch Size (Another Dimension)\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # Number of independent sequences can we process in parallel\n",
    "block_size = 8 # Maximum context length for our predictions\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train if split == 'train' else validation\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) # randomly generated 4 numbers b/w 0 and block size\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) # get chunks for all of the ix\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # get the target for the tensors\n",
    "    return x,y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "# 32 total examples in a single batch of size 4\n",
    "for b in range(batch_size): # Batch Dimension\n",
    "    for t in range(block_size): # Time Dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} then the target:{target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3180b490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(65, 65)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Embedding(vocab_size, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "deab63b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self,vocab_size):\n",
    "        \n",
    "        super().__init__()\n",
    "        # each token reads off the logits for the next token from lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are both (B,T) tensors of integers\n",
    "        \n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "        \n",
    "        if targets == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B,T,C = logits.shape # Reshaping\n",
    "\n",
    "            logits = logits.view(B*T,C)\n",
    "\n",
    "            targets = targets.view(B*T)\n",
    "\n",
    "            loss = F.cross_entropy(logits,targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B,T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last step -> bigram\n",
    "            logits = logits[:,-1,:] # Makes it (B,C)\n",
    "            # apply softmax\n",
    "            probs = F.softmax(logits, dim=-1) # (B,C)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B,1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        \n",
    "        return idx\n",
    "            \n",
    "            \n",
    "    \n",
    "m = BigramLanguageModel(vocab_size)\n",
    "# we are predicting what comes next based on individual identity of a single token\n",
    "logits,loss = m(xb,yb)\n",
    "print(logits.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9231971e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9186a4f4",
   "metadata": {},
   "source": [
    "# totally random modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d18578d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nrVchSFUIdd q?sPJpUdhMCK$VXXevXJFMl,i\\nYxA:gWId,EXR,iMC,$?srV$VztRwb?KpgUWFjR$zChOLm;JrDnDph\\nLBj,KZxJa'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(m.generate(idx, max_new_tokens = 100)[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c8649982",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1.2e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2b0363b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.439894199371338\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "for steps in range(10000):\n",
    "    xb,yb = get_batch('train')\n",
    "    \n",
    "    logits, loss = m(xb, yb)\n",
    "    \n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cb69a3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "YO, t milll ll thepa'djo henose Fomevofan GLOfoouloredoolu ket.\n",
      "Wine d urus, lomere by,\n",
      "ADod at alse\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx, max_new_tokens = 100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72be71a3",
   "metadata": {},
   "source": [
    "### printing french lmao"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01baf736",
   "metadata": {},
   "source": [
    "Karapthy's math trick for self attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "791e27ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1808, -0.0700],\n",
       "         [-0.3596, -0.9152],\n",
       "         [ 0.6258,  0.0255],\n",
       "         [ 0.9545,  0.0643],\n",
       "         [ 0.3612,  1.1679],\n",
       "         [-1.3499, -0.5102],\n",
       "         [ 0.2360, -0.2398],\n",
       "         [-0.9211,  1.5433]],\n",
       "\n",
       "        [[ 1.3488, -0.1396],\n",
       "         [ 0.2858,  0.9651],\n",
       "         [-2.0371,  0.4931],\n",
       "         [ 1.4870,  0.5910],\n",
       "         [ 0.1260, -1.5627],\n",
       "         [-1.1601, -0.3348],\n",
       "         [ 0.4478, -0.8016],\n",
       "         [ 1.5236,  2.5086]],\n",
       "\n",
       "        [[-0.6631, -0.2513],\n",
       "         [ 1.0101,  0.1215],\n",
       "         [ 0.1584,  1.1340],\n",
       "         [-1.1539, -0.2984],\n",
       "         [-0.5075, -0.9239],\n",
       "         [ 0.5467, -1.4948],\n",
       "         [-1.2057,  0.5718],\n",
       "         [-0.5974, -0.6937]],\n",
       "\n",
       "        [[ 1.6455, -0.8030],\n",
       "         [ 1.3514, -0.2759],\n",
       "         [-1.5108,  2.1048],\n",
       "         [ 2.7630, -1.7465],\n",
       "         [ 1.4516, -1.5103],\n",
       "         [ 0.8212, -0.2115],\n",
       "         [ 0.7789,  1.5333],\n",
       "         [ 1.6097, -0.4032]]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "54970e2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.9211,  1.5433])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0][7] # 2 channels and 8 time components (8 tokens in a batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eca7537",
   "metadata": {},
   "source": [
    "The token should only flow information backwards, do an average of all the tokens before, which summarizies the information behind, but this is lossy but works ish for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "29bd163e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag = torch.zeros((B,T,C))\n",
    "\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        prev = x[b,:t+1]\n",
    "        bag[b,t] = torch.mean(prev,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "974fb3a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "c79228ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.3596, -0.9152],\n",
       "        [ 0.6258,  0.0255],\n",
       "        [ 0.9545,  0.0643],\n",
       "        [ 0.3612,  1.1679],\n",
       "        [-1.3499, -0.5102],\n",
       "        [ 0.2360, -0.2398],\n",
       "        [-0.9211,  1.5433]])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ad5876",
   "metadata": {},
   "source": [
    "we can achieve this efficiently using mathematication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "de6a0796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tril(torch.ones(3,3))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "640a8b7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = a / torch.sum(a,1, keepdim=True)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e9862c",
   "metadata": {},
   "source": [
    "Just doing regular average of all words before the target word in the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c3a4ca3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wei = torch.tril(torch.ones(T,T))\n",
    "wei = wei / torch.sum(wei,1, keepdim=True)\n",
    "bag2 = wei @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "0e6f2bf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(bag2,bag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5587d367",
   "metadata": {},
   "source": [
    "### Self attention mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559a11ce",
   "metadata": {},
   "source": [
    "Now we do not want all the tokens to be the same importance, some might be more important, so we will add key, query in the tranformer. Below we will implement the single head for self attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "77f42962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# head size is a hyper parameter\n",
    "head_size = 16\n",
    "key = nn.Linear(C,head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "\n",
    "k = key(x) # (B,T,16)\n",
    "q = query(x)  # (B,T,16)\n",
    "\n",
    "wei = q @ k.transpose(-2,-1) # transpose the last two dimensions (B, T, 16) @ (B, 16, T) --> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) * (head_size ** -0.5)\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "\n",
    "out = wei @ v\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "98567a30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3966, 0.6034, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3069, 0.2892, 0.4039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3233, 0.2175, 0.2443, 0.2149, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1479, 0.2034, 0.1663, 0.1455, 0.3369, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1259, 0.2490, 0.1324, 0.1062, 0.3141, 0.0724, 0.0000, 0.0000],\n",
       "         [0.1598, 0.1990, 0.1140, 0.1125, 0.1418, 0.1669, 0.1061, 0.0000],\n",
       "         [0.0845, 0.1197, 0.1078, 0.1537, 0.1086, 0.1146, 0.1558, 0.1553]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4016, 0.5984, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3365, 0.2271, 0.4364, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3019, 0.2060, 0.2899, 0.2022, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1058, 0.1700, 0.1530, 0.3451, 0.2261, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1526, 0.1645, 0.1357, 0.2684, 0.1919, 0.0869, 0.0000, 0.0000],\n",
       "         [0.1103, 0.1711, 0.0761, 0.1654, 0.1667, 0.1643, 0.1461, 0.0000],\n",
       "         [0.1770, 0.1063, 0.1198, 0.0943, 0.1697, 0.1205, 0.1052, 0.1073]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4955, 0.5045, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2861, 0.3657, 0.3483, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1242, 0.3939, 0.1981, 0.2838, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3531, 0.1668, 0.1768, 0.1813, 0.1220, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1553, 0.1779, 0.1492, 0.1539, 0.1723, 0.1914, 0.0000, 0.0000],\n",
       "         [0.0722, 0.1255, 0.1119, 0.1896, 0.1537, 0.1918, 0.1552, 0.0000],\n",
       "         [0.1344, 0.1368, 0.0970, 0.1395, 0.1292, 0.1304, 0.0790, 0.1535]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5351, 0.4649, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3776, 0.4907, 0.1317, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3079, 0.2849, 0.2206, 0.1865, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2074, 0.2611, 0.1368, 0.2071, 0.1876, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1733, 0.3004, 0.0656, 0.1682, 0.1669, 0.1255, 0.0000, 0.0000],\n",
       "         [0.1216, 0.1213, 0.1416, 0.1119, 0.1439, 0.2213, 0.1383, 0.0000],\n",
       "         [0.0925, 0.1598, 0.0945, 0.1355, 0.1356, 0.1086, 0.1185, 0.1548]]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "76929262",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, block_size,head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(C,head_size, bias=False)\n",
    "        self.query = nn.Linear(C, head_size, bias=False)\n",
    "        self.value = nn.Linear(C, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size,block_size)))\n",
    "    \n",
    "    def forward(self,x):\n",
    "        B,T,C = x.shape\n",
    "        k = key(x) # (B,T,C)\n",
    "        q = query(x)  # (B,T,C)\n",
    "        wei = q @ k.transpose(-2,-1) # transpose the last two dimensions (B, T, 16) @ (B, 16, T) --> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T,:T] == 0, float('-inf')) * (head_size ** -0.5)\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        v = value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "3b2ffb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([64, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54],\n",
      "        [57, 43, 60, 43, 52,  1, 63, 43],\n",
      "        [60, 43, 42,  8,  0, 25, 63,  1],\n",
      "        [56, 42,  5, 57,  1, 57, 39, 49],\n",
      "        [43, 57, 58, 63,  6,  1, 58, 46],\n",
      "        [43,  1, 51, 39, 63,  1, 40, 43],\n",
      "        [58, 46, 43,  1, 43, 39, 56, 57],\n",
      "        [39, 58, 47, 53, 52, 12,  1, 37],\n",
      "        [53, 56, 43,  1, 21,  1, 41, 39],\n",
      "        [50, 39, 52, 63,  1, 47, 58, 57],\n",
      "        [56, 53, 63,  1, 42, 47, 42,  1],\n",
      "        [39, 51,  1, 39, 44, 56, 39, 47],\n",
      "        [17, 24, 21, 38, 13, 14, 17, 32],\n",
      "        [ 1, 39, 52, 42,  1, 45, 43, 50],\n",
      "        [ 1, 58, 46, 39, 58,  1, 42, 53],\n",
      "        [ 1, 61, 53, 59, 50, 42,  1, 21],\n",
      "        [59, 57, 40, 39, 52, 42,  1, 40],\n",
      "        [52, 42,  8,  0,  0, 23, 21, 26],\n",
      "        [45, 53, 42, 57,  0, 23, 43, 43],\n",
      "        [52,  1, 61, 39, 57,  1, 51, 53],\n",
      "        [39, 49, 12,  1, 27,  1, 58, 56],\n",
      "        [53, 44,  1, 57, 54, 43, 43, 41],\n",
      "        [57, 53, 52, 57,  8,  0,  0, 25],\n",
      "        [ 1, 42, 43, 44, 43, 41, 58,  1],\n",
      "        [21,  1, 61, 39, 52, 42, 43, 56],\n",
      "        [43, 43, 51,  5, 42,  1, 40, 59],\n",
      "        [45, 50, 63,  1, 52, 53, 61, 12],\n",
      "        [52, 53, 58,  8,  0, 25, 63,  1],\n",
      "        [53, 58,  6,  1, 51, 63,  1, 50],\n",
      "        [52, 57,  8,  0, 21,  5, 50, 50],\n",
      "        [47, 58, 46,  1, 63, 53, 59,  6],\n",
      "        [ 1, 53, 40, 43, 63, 12,  0, 26],\n",
      "        [50, 42, 57, 58,  6,  0, 32, 46],\n",
      "        [ 1, 21,  1, 51, 39, 63,  7,  7],\n",
      "        [ 1, 59, 54,  1, 40, 63,  1, 19],\n",
      "        [43,  1, 43, 52, 53, 59, 45, 46],\n",
      "        [42, 10,  1, 61, 43,  1, 41, 39],\n",
      "        [20, 53, 61,  1, 44, 53, 52, 42],\n",
      "        [58, 43, 56, 51, 47, 52, 43, 57],\n",
      "        [43,  1, 53, 44,  1, 58, 56, 59],\n",
      "        [43, 52,  1, 44, 56, 53, 51,  1],\n",
      "        [11,  1, 47, 58,  1, 47, 57,  1],\n",
      "        [56, 47, 58,  1, 61, 43, 56, 43],\n",
      "        [47, 43, 58,  6,  1, 53, 52,  1],\n",
      "        [40, 56, 53, 58, 46, 43, 56,  1],\n",
      "        [59, 56,  1, 53, 44,  1, 58, 46],\n",
      "        [13, 52, 42,  1, 57, 39, 47, 42],\n",
      "        [45, 39, 56, 50, 39, 52, 42,  8],\n",
      "        [43, 39, 56, 57,  6,  0, 13, 52],\n",
      "        [ 8,  0,  0, 17, 16, 35, 13, 30],\n",
      "        [47, 57,  1, 58, 46, 43,  1, 61],\n",
      "        [ 1, 52, 47, 43, 41, 43,  1, 28],\n",
      "        [59, 54,  1, 58, 46, 43,  1, 63],\n",
      "        [47, 58,  1, 57, 43, 43, 51, 57],\n",
      "        [43, 50,  1, 51, 43,  1, 58, 53],\n",
      "        [39, 50, 50,  1, 42, 43, 57, 54],\n",
      "        [ 1, 54, 56, 39, 63,  1, 63, 53],\n",
      "        [43, 57, 58,  6,  1, 53, 44,  1],\n",
      "        [53,  6,  1, 61, 46, 43, 52,  1],\n",
      "        [ 1, 58, 56, 59, 43,  1, 44, 43],\n",
      "        [39, 58,  1, 21,  1, 57, 46, 39]])\n",
      "targets:\n",
      "torch.Size([64, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39],\n",
      "        [43, 60, 43, 52,  1, 63, 43, 39],\n",
      "        [43, 42,  8,  0, 25, 63,  1, 45],\n",
      "        [42,  5, 57,  1, 57, 39, 49, 43],\n",
      "        [57, 58, 63,  6,  1, 58, 46, 47],\n",
      "        [ 1, 51, 39, 63,  1, 40, 43,  1],\n",
      "        [46, 43,  1, 43, 39, 56, 57, 10],\n",
      "        [58, 47, 53, 52, 12,  1, 37, 53],\n",
      "        [56, 43,  1, 21,  1, 41, 39, 51],\n",
      "        [39, 52, 63,  1, 47, 58, 57, 43],\n",
      "        [53, 63,  1, 42, 47, 42,  1, 57],\n",
      "        [51,  1, 39, 44, 56, 39, 47, 42],\n",
      "        [24, 21, 38, 13, 14, 17, 32, 20],\n",
      "        [39, 52, 42,  1, 45, 43, 50, 42],\n",
      "        [58, 46, 39, 58,  1, 42, 53,  1],\n",
      "        [61, 53, 59, 50, 42,  1, 21,  1],\n",
      "        [57, 40, 39, 52, 42,  1, 40, 47],\n",
      "        [42,  8,  0,  0, 23, 21, 26, 19],\n",
      "        [53, 42, 57,  0, 23, 43, 43, 54],\n",
      "        [ 1, 61, 39, 57,  1, 51, 53, 56],\n",
      "        [49, 12,  1, 27,  1, 58, 56, 39],\n",
      "        [44,  1, 57, 54, 43, 43, 41, 46],\n",
      "        [53, 52, 57,  8,  0,  0, 25, 17],\n",
      "        [42, 43, 44, 43, 41, 58,  1, 53],\n",
      "        [ 1, 61, 39, 52, 42, 43, 56,  6],\n",
      "        [43, 51,  5, 42,  1, 40, 59, 56],\n",
      "        [50, 63,  1, 52, 53, 61, 12,  0],\n",
      "        [53, 58,  8,  0, 25, 63,  1, 61],\n",
      "        [58,  6,  1, 51, 63,  1, 50, 53],\n",
      "        [57,  8,  0, 21,  5, 50, 50,  1],\n",
      "        [58, 46,  1, 63, 53, 59,  6,  1],\n",
      "        [53, 40, 43, 63, 12,  0, 26, 39],\n",
      "        [42, 57, 58,  6,  0, 32, 46, 43],\n",
      "        [21,  1, 51, 39, 63,  7,  7, 58],\n",
      "        [59, 54,  1, 40, 63,  1, 19, 53],\n",
      "        [ 1, 43, 52, 53, 59, 45, 46, 11],\n",
      "        [10,  1, 61, 43,  1, 41, 39, 52],\n",
      "        [53, 61,  1, 44, 53, 52, 42, 50],\n",
      "        [43, 56, 51, 47, 52, 43, 57,  0],\n",
      "        [ 1, 53, 44,  1, 58, 56, 59, 43],\n",
      "        [52,  1, 44, 56, 53, 51,  1, 39],\n",
      "        [ 1, 47, 58,  1, 47, 57,  1, 39],\n",
      "        [47, 58,  1, 61, 43, 56, 43,  1],\n",
      "        [43, 58,  6,  1, 53, 52,  1, 32],\n",
      "        [56, 53, 58, 46, 43, 56,  1, 47],\n",
      "        [56,  1, 53, 44,  1, 58, 46, 43],\n",
      "        [52, 42,  1, 57, 39, 47, 42,  1],\n",
      "        [39, 56, 50, 39, 52, 42,  8,  1],\n",
      "        [39, 56, 57,  6,  0, 13, 52, 42],\n",
      "        [ 0,  0, 17, 16, 35, 13, 30, 16],\n",
      "        [57,  1, 58, 46, 43,  1, 61, 47],\n",
      "        [52, 47, 43, 41, 43,  1, 28, 50],\n",
      "        [54,  1, 58, 46, 43,  1, 63, 43],\n",
      "        [58,  1, 57, 43, 43, 51, 57,  1],\n",
      "        [50,  1, 51, 43,  1, 58, 53,  1],\n",
      "        [50, 50,  1, 42, 43, 57, 54, 39],\n",
      "        [54, 56, 39, 63,  1, 63, 53, 59],\n",
      "        [57, 58,  6,  1, 53, 44,  1, 61],\n",
      "        [ 6,  1, 61, 46, 43, 52,  1, 58],\n",
      "        [58, 56, 59, 43,  1, 44, 43, 39],\n",
      "        [58,  1, 21,  1, 57, 46, 39, 50]])\n",
      "----\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (512x16 and 32x65)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[202], line 78\u001b[0m\n\u001b[1;32m     76\u001b[0m m \u001b[38;5;241m=\u001b[39m BigramLanguageModel(vocab_size)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# we are predicting what comes next based on individual identity of a single token\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m logits,loss \u001b[38;5;241m=\u001b[39m m(xb,yb)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28mprint\u001b[39m(logits\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[202], line 22\u001b[0m, in \u001b[0;36mBigramLanguageModel.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m     20\u001b[0m x \u001b[38;5;241m=\u001b[39m token_embeddings \u001b[38;5;241m+\u001b[39m position_embeddings\n\u001b[1;32m     21\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msa_head(x) \u001b[38;5;66;03m# apply self attention head here\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(x)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (512x16 and 32x65)"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "n_embed = 32\n",
    "block_size = 8\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self,vocab_size):\n",
    "        \n",
    "        super().__init__()\n",
    "        # each token reads off the logits for the next token from lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "        self.sa_head = Head(block_size=block_size,head_size=n_embed)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are both (B,T) tensors of integers\n",
    "        token_embeddings = self.token_embedding_table(idx) # (B,T,C)\n",
    "        position_embeddings = self.position_embedding_table(torch.arange(idx.shape[1]))\n",
    "        x = token_embeddings + position_embeddings\n",
    "        x = self.sa_head(x) # apply self attention head here\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        if targets == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B,T,C = logits.shape # Reshaping\n",
    "\n",
    "            logits = logits.view(B*T,C)\n",
    "\n",
    "            targets = targets.view(B*T)\n",
    "\n",
    "            loss = F.cross_entropy(logits,targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B,T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            \n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            \n",
    "            # predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last step -> bigram\n",
    "            logits = logits[:,-1,:] # Makes it (B,C)\n",
    "            # apply softmax\n",
    "            probs = F.softmax(logits, dim=-1) # (B,C)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B,1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        \n",
    "        return idx\n",
    "\n",
    "# Batch Size (Another Dimension)\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train if split == 'train' else validation\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) # randomly generated 4 numbers b/w 0 and block size\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) # get chunks for all of the ix\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # get the target for the tensors\n",
    "    return x,y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "    \n",
    "m = BigramLanguageModel(vocab_size)\n",
    "# we are predicting what comes next based on individual identity of a single token\n",
    "logits,loss = m(xb,yb)\n",
    "print(logits.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a424e8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4b0c74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
